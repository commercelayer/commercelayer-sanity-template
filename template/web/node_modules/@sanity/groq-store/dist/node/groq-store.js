function _interopDefault (ex) { return (ex && (typeof ex === 'object') && 'default' in ex) ? ex['default'] : ex; }

var EventSource = _interopDefault(require('eventsource'));
var groq = _interopDefault(require('groq'));
var deepEqual = _interopDefault(require('fast-deep-equal'));
var throttleDebounce = require('throttle-debounce');
var groqJs = require('groq-js');
var mendoza = require('mendoza');
var split = _interopDefault(require('split2'));
var get = _interopDefault(require('simple-get'));

function _extends() {
  _extends = Object.assign || function (target) {
    for (var i = 1; i < arguments.length; i++) {
      var source = arguments[i];

      for (var key in source) {
        if (Object.prototype.hasOwnProperty.call(source, key)) {
          target[key] = source[key];
        }
      }
    }

    return target;
  };

  return _extends.apply(this, arguments);
}

function listen(EventSourceImpl, config, handlers) {
  const {
    projectId,
    dataset,
    token
  } = config;
  const headers = token ? {
    Authorization: `Bearer ${token}`
  } : undefined;
  const url = `https://${projectId}.api.sanity.io/v1/data/listen/${dataset}?query=*&effectFormat=mendoza`;
  const es = new EventSourceImpl(url, {
    withCredentials: true,
    headers
  });
  es.addEventListener('welcome', handlers.open, false);
  es.addEventListener('mutation', getMutationParser(handlers.next), false);
  es.addEventListener('channelError', msg => {
    es.close();
    let data;

    try {
      data = JSON.parse(msg.data);
    } catch (err) {
      handlers.error(new Error('Unknown error parsing listener message'));
      return;
    }

    handlers.error(new Error(data.message || data.error || `Listener returned HTTP ${data.statusCode}`));
  }, false);
  es.addEventListener('error', () => {
    const origin = typeof window !== 'undefined' && window.location.origin;
    const hintSuffix = origin ? `, and that the CORS-origin (${origin}) is allowed` : '';
    handlers.error(new Error(`Error establishing listener - check that the project ID and dataset are correct${hintSuffix}`));
  }, false);
  return {
    unsubscribe: () => Promise.resolve(es.close())
  };
}

function getMutationParser(cb) {
  return msg => {
    let data;

    try {
      data = JSON.parse(msg.data);
    } catch (err) {
      // intentional noop
      return;
    }

    cb(data);
  };
}

function isDraft(doc) {
  return doc._id.startsWith('drafts.');
}
function getPublishedId(document) {
  return isDraft(document) ? document._id.slice(7) : document._id;
}

function applyPatchWithoutRev(doc, patch) {
  const patchDoc = _extends({}, doc);

  delete patchDoc._rev;
  return mendoza.applyPatch(patchDoc, patch);
}

const DEBOUNCE_MS = 25;

function noop() {
  return Promise.resolve();
}

function getSyncingDataset(config, onNotifyUpdate, {
  getDocuments,
  EventSource
}) {
  const {
    projectId,
    dataset,
    listen: useListener,
    overlayDrafts,
    documentLimit
  } = config;

  if (!useListener) {
    const loaded = getDocuments({
      projectId,
      dataset,
      documentLimit
    }).then(onUpdate).then(noop);
    return {
      unsubscribe: noop,
      loaded
    };
  }

  const indexedDocuments = new Map(); // undefined until the listener has been set up and the initial export is done

  let documents; // holds any mutations that happen while fetching documents so they can be applied after updates

  const buffer = []; // Return a promise we can resolve once we've established a listener and reconciled any mutations

  let onDoneLoading;
  let onLoadError;
  const loaded = new Promise((resolve, reject) => {
    onDoneLoading = resolve;
    onLoadError = reject;
  }); // We don't want to flush updates while we're in the same transaction, so a normal
  // throttle/debounce wouldn't do it. We need to wait and see if the next mutation is
  // within the same transaction as the previous, and if not we can flush. Of course,
  // we can't wait forever, so an upper threshold of X ms should be counted as "ok to flush"

  let stagedDocs;
  let previousTrx;
  let flushTimeout;
  const listener = listen(EventSource, config, {
    next: onMutationReceived,
    open: onOpen,
    error: error => onLoadError(error)
  });
  return {
    unsubscribe: listener.unsubscribe,
    loaded
  };

  async function onOpen() {
    const initial = await getDocuments({
      projectId,
      dataset,
      documentLimit
    });
    documents = applyBufferedMutations(initial, buffer);
    documents.forEach(doc => indexedDocuments.set(doc._id, doc));
    onUpdate(documents);
    onDoneLoading();
  }

  function onMutationReceived(msg) {
    if (documents) {
      applyMutation(msg);
      scheduleUpdate(documents, msg);
    } else {
      buffer.push(msg);
    }
  }

  function scheduleUpdate(docs, msg) {
    clearTimeout(flushTimeout);

    if (previousTrx !== msg.transactionId && stagedDocs) {
      // This is a new transaction, meaning we can immediately flush any pending
      // doc updates if there are any
      onUpdate(stagedDocs);
      previousTrx = undefined;
    } else {
      previousTrx = msg.transactionId;
      stagedDocs = docs.slice();
    }

    flushTimeout = setTimeout(onUpdate, DEBOUNCE_MS, docs.slice());
  }

  function onUpdate(docs) {
    stagedDocs = undefined;
    flushTimeout = undefined;
    previousTrx = undefined;
    onNotifyUpdate(overlayDrafts ? overlay(docs) : docs);
  }

  function applyMutation(msg) {
    if (!msg.effects || msg.documentId.startsWith('_.')) {
      return;
    }

    const document = indexedDocuments.get(msg.documentId) || null;
    replaceDocument(msg.documentId, applyPatchWithoutRev(document, msg.effects.apply));
  }

  function replaceDocument(id, document) {
    const current = indexedDocuments.get(id);
    const docs = documents || [];
    const position = current ? docs.indexOf(current) : -1;

    if (position === -1 && document) {
      // Didn't exist previously, but was now created. Add it.
      docs.push(document);
      indexedDocuments.set(id, document);
    } else if (document) {
      // Existed previously and still does. Replace it.
      docs.splice(position, 1, document);
      indexedDocuments.set(id, document);
    } else {
      // Existed previously, but is now deleted. Remove it.
      docs.splice(position, 1);
      indexedDocuments.delete(id);
    }
  }
}

function applyBufferedMutations(documents, mutations) {
  // Group by document ID
  const groups = new Map();
  mutations.forEach(mutation => {
    const group = groups.get(mutation.documentId) || [];
    group.push(mutation);
    groups.set(mutation.documentId, group);
  }); // Discard all mutations that happened before our current document

  groups.forEach((group, id) => {
    const document = documents.find(doc => doc._id === id);

    if (!document) {
      // @todo handle
      // eslint-disable-next-line no-console
      console.warn('Received mutation for missing document %s', id);
      return;
    } // Mutations are sorted by timestamp, apply any that arrived after
    // we fetched the initial documents


    let hasFoundRevision = false;
    let current = document;
    group.forEach(mutation => {
      hasFoundRevision = hasFoundRevision || mutation.previousRev === document._rev;

      if (!hasFoundRevision) {
        return;
      }

      if (mutation.effects) {
        current = applyPatchWithoutRev(current, mutation.effects.apply);
      }
    }); // Replace the indexed documents

    documents.splice(documents.indexOf(document), 1, current);
  });
  return documents;
}

function overlay(documents) {
  const overlayed = new Map();
  documents.forEach(doc => {
    const existing = overlayed.get(getPublishedId(doc));

    if (doc._id.startsWith('drafts.')) {
      // Drafts always overlay
      overlayed.set(getPublishedId(doc), pretendThatItsPublished(doc));
    } else if (!existing) {
      // Published documents only override if draft doesn't exist
      overlayed.set(doc._id, doc);
    }
  });
  return Array.from(overlayed.values());
} // Strictly speaking it would be better to allow groq-js to resolve `draft.<id>`,
// but for now this will have to do


function pretendThatItsPublished(doc) {
  return _extends({}, doc, {
    _id: getPublishedId(doc)
  });
}

function groqStore(config, envImplementations) {
  let documents = [];
  const executeThrottled = throttleDebounce.throttle(config.subscriptionThrottleMs || 50, executeAllSubscriptions);
  const activeSubscriptions = [];
  let dataset;

  async function loadDataset() {
    if (!dataset) {
      dataset = getSyncingDataset(config, docs => {
        documents = docs;
        executeThrottled();
      }, envImplementations);
    }

    await dataset.loaded;
  }

  async function query(groqQuery, params) {
    await loadDataset();
    const tree = groqJs.parse(groqQuery);
    const result = await groqJs.evaluate(tree, {
      dataset: documents,
      params
    });
    return result.get();
  }

  async function getDocument(documentId) {
    await loadDataset();
    return query(groq`*[_id == $id][0]`, {
      id: documentId
    });
  }

  async function getDocuments(documentIds) {
    await loadDataset();
    const subQueries = documentIds.map(id => `*[_id == "${id}"][0]`).join(',\n');
    return query(`[${subQueries}]`);
  }

  function subscribe(groqQuery, params, callback) {
    if (!config.listen) {
      throw new Error('Cannot use `subscribe()` without `listen: true`');
    } // @todo Execute the query against an empty dataset for validation purposes
    // Store the subscription so we can re-run the query on new data


    const subscription = {
      query: groqQuery,
      params,
      callback
    };
    activeSubscriptions.push(subscription);
    let unsubscribed = false;

    const unsubscribe = () => {
      if (unsubscribed) {
        return Promise.resolve();
      }

      unsubscribed = true;
      activeSubscriptions.splice(activeSubscriptions.indexOf(subscription), 1);
      return Promise.resolve();
    };

    executeQuerySubscription(subscription);
    return {
      unsubscribe
    };
  }

  function executeQuerySubscription(subscription) {
    return query(subscription.query, subscription.params).then(res => {
      if ('previousResult' in subscription && deepEqual(subscription.previousResult, res)) {
        return;
      }

      subscription.previousResult = res;
      subscription.callback(undefined, res);
    }).catch(err => {
      subscription.callback(err);
    });
  }

  function executeAllSubscriptions() {
    activeSubscriptions.forEach(executeQuerySubscription);
  }

  function close() {
    executeThrottled.cancel();
    return dataset ? dataset.unsubscribe() : Promise.resolve();
  }

  return {
    query,
    getDocument,
    getDocuments,
    subscribe,
    close
  };
}

function isStreamError(result) {
  if (!result) {
    return false;
  }

  if (!('error' in result) || typeof result.error !== 'object' || result.error === null) {
    return false;
  }

  return 'description' in result.error && typeof result.error.description === 'string' && !('_id' in result);
}
function getError(body) {
  if (typeof body === 'object' && 'error' in body && 'message' in body) {
    return body.message || body.error;
  }

  return '<unknown error>';
}
function isRelevantDocument(doc) {
  return !doc._id.startsWith('_.');
}

const getDocuments = function getDocuments({
  projectId,
  dataset,
  token,
  documentLimit
}) {
  const headers = token ? {
    Authorization: `Bearer ${token}`
  } : undefined;
  return new Promise((resolve, reject) => {
    get({
      url: `https://${projectId}.api.sanity.io/v1/data/export/${dataset}`,
      headers
    }, (err, response) => {
      if (err) {
        reject(err);
        return;
      }

      response.setEncoding('utf8');
      const chunks = [];

      if (response.statusCode !== 200) {
        response.on('data', chunk => chunks.push(chunk)).on('end', () => {
          const body = JSON.parse(Buffer.concat(chunks).toString('utf8'));
          reject(new Error(`Error streaming dataset: ${getError(body)}`));
        });
        return;
      }

      const documents = [];
      response.pipe(split(JSON.parse)).on('data', doc => {
        if (isStreamError(doc)) {
          reject(new Error(`Error streaming dataset: ${doc.error}`));
          return;
        }

        if (doc && isRelevantDocument(doc)) {
          documents.push(doc);
        }

        if (documentLimit && documents.length > documentLimit) {
          reject(new Error(`Error streaming dataset: Reached limit of ${documentLimit} documents`));
          response.destroy();
        }
      }).on('end', () => resolve(documents));
    });
  });
};

function assertEnvSupport() {
  const [major] = process.version.replace(/^v/, '').split('.', 1).map(Number);

  if (major < 10) {
    throw new Error('Node.js version 10 or higher required');
  }
}

/**
 * Note: Entry point for _browser_ build is in browser/index.ts
 */
function groqStore$1(config) {
  assertEnvSupport();
  return groqStore(config, {
    EventSource: EventSource,
    getDocuments
  });
}

exports.groq = groq;
exports.groqStore = groqStore$1;
//# sourceMappingURL=groq-store.js.map
